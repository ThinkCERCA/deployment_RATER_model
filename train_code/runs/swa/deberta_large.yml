max_length: 1792
mask: 0.2
skip_validation: 4
epochs: 6
model:
  dropout: 0.2
  dynamic_positive: true
  dropout_eval: false
  pretrained: true
  model_name: microsoft/deberta-large
  gradient_checkpointing: false
  use_awp: false
  skip_awp: 0
optimizer:
  name: torch.optim.AdamW
  weight_decay: 0.01
  head_lr_factor: 5
  params:
    lr: 1.0e-05
    eps: 1.0e-08
    betas: [0.9, 0.999]
scheduler:
  name: cosine
  params:
    num_cycles: 0.4
  interval: step
  warmup: 0.05
train_loader:
  batch_size: 1
  drop_last: true
  num_workers: 2
  pin_memory: false
  shuffle: true
val_loader:
  batch_size: 1
  drop_last: false
  num_workers: 2
  pin_memory: false
  shuffle: false
trainer:
  devices: 1
  accumulate_grad_batches: 2
  fast_dev_run: false
  num_sanity_val_steps: 0
  #precision: 16-mixed
  val_check_interval: 0.2
  gradient_clip_val: 1.0